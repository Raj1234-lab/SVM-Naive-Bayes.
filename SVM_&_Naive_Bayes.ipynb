{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "4Vq7p6YKukwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:Information Gain measures how much uncertainty (entropy) in the data is reduced after splitting it based on a particular feature.\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "It tells us how well a given attribute separates the training examples according to their target classification.\n",
        "\n",
        "Information Gain (IG) is a key concept used in Decision Trees (such as ID3, C4.5, and CART algorithms) to decide which feature to split on at each step of building the tree.\n",
        "\n",
        "Information gain = Entopy(parent) - submition of (prportion of sample in subset * entropy)\n",
        "\n",
        "Where:\n",
        "\n",
        "ð‘†\n",
        "S: the original dataset\n",
        "\n",
        "S\n",
        "i\n",
        "\t: subsets after splitting on an attribute\n",
        "\tâ€‹\n",
        "\n",
        "âˆ£/âˆ£Sâˆ£: proportion of samples in subset\n",
        "ð‘†\n",
        "ð‘–\n",
        "S\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "Entropy(ð‘†)=âˆ’submition of ð‘ð‘– log2(ð‘ð‘–)\n",
        "\n",
        "Entropy(S)=âˆ’âˆ‘pi\n",
        "log\n",
        "2\n",
        "(pi), where\n",
        "\n",
        "pi is the probability of class ð‘–\n"
      ],
      "metadata": {
        "id": "ohMsP450u2uO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases"
      ],
      "metadata": {
        "id": "O-VqStKvxmUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Entropy comes from information theory â€” it tells how much information (or surprise) there is in the data.\n",
        "\n",
        "Gini Impurity is a simpler metric that tells how often a randomly chosen sample would be misclassified if it were labeled randomly according to class distribution.\n",
        "\n",
        "Entropy - From 0 (pure node) to 1 (maximum uncertainty).\n",
        "Gini Impurity - From 0 (pure node) to 0.5 (maximum impurity).\n",
        "Entropy - To select the feature that results in the largest drop in impurity after a split.\n",
        "\n",
        "Gini impurity - To select the feature that provides the highest \"Information Gain,\" or the greatest reduction in uncertainty.\n",
        "\n",
        "Entropy - To select the feature that provides the highest \"Information Gain,\" or the greatest reduction in uncertainty.\n",
        "\n",
        "Can sometimes produce slightly better results, though the difference is often negligible.\n",
        "\n",
        "Can be more sensitive to imbalanced class distributions and may lead to deeper, more balanced trees.\n",
        "\n",
        "Gini impurity - Faster to compute because it does not involve logarithmic calculations.\n",
        "\n",
        "Preferred for very large datasets where computational efficiency is a key concern.\n",
        "\n",
        "Tends to favor splits that create more balanced partitions."
      ],
      "metadata": {
        "id": "wi83RJx3x5K-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "SFF7f8MO0Iu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Pre-pruning, or early stopping, is a technique in decision trees that halts the growth of a tree before it becomes fully developed to prevent overfitting. It stops the tree-building process based on certain criteria, such as reaching a maximum depth, having too few samples in a node, or not achieving a minimum impurity decrease from a split. This approach restricts the complexity of the model from the beginning, leading to a smaller, simpler tree that may generalize better to new data.\n",
        "\n",
        "Maximum depth: The tree has reached a predefined maximum number of levels.\n",
        "\n",
        "Minimum samples: A node has fewer than a specified minimum number of samples.\n",
        "\n",
        "Minimum impurity decrease: The best split at a node doesn't result in a significant enough improvement in the impurity (e.g., Gini impurity or information gain).\n",
        "\n",
        "Benefits:\n",
        "\n",
        "Faster training: It is computationally more efficient than allowing the tree to grow completely and then trim it later.\n",
        "\n",
        "Prevents overfitting: By limiting complexity early on, it reduces the risk of overfitting the training data.\n"
      ],
      "metadata": {
        "id": "x-50uI2zz8Ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_."
      ],
      "metadata": {
        "id": "etQ4bDTl0uV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer"
      ],
      "metadata": {
        "id": "_zUGsPgo04SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INI0ixXUzk79",
        "outputId": "4dcd3042-ac58-4bc4-e427-d273a2eacbef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n"
      ],
      "metadata": {
        "id": "FNDxL7a-1VZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks â€” most commonly for binary classification.\n",
        "\n",
        "SVM tries to draw a line (in 2D) or a hyperplane (in higher dimensions) that divides the data into classes as clearly as possible â€” while keeping the widest possible gap between them.\n",
        "\n",
        "That gap is called the margin.\n",
        "\n",
        "The data points closest to the boundary are called Support Vectors â€” they define the position and orientation of the hyperplane.\n",
        "\n",
        "Types of SVM:\n",
        "\n",
        "Linear SVM:\n",
        "Used when the data is linearly separable (can be divided by a straight line or plane).\n",
        "\n",
        "Non-Linear SVM:\n",
        "Uses a Kernel Function (like polynomial or RBF) to map data into a higher-dimensional space where a linear separator can be found."
      ],
      "metadata": {
        "id": "nNWGXzMm1kHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "Rr6UzeB6146f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The Kernel Trick is one of the most powerful concepts in Support Vector Machines (SVMs) â€” it allows SVMs to handle non-linear data efficiently without explicitly transforming it into a higher-dimensional space.\n",
        "\n",
        "Instead of manually converting data to a higher dimension, the Kernel Trick uses a mathematical function (kernel) to compute the similarity between data points in that higher-dimensional space â€” without ever performing the transformation explicitly.\n",
        "\n",
        "Normally, to separate non-linear data, we transform data points\n",
        "ð‘¥\n",
        "x using a mapping function\n",
        "ðœ™\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "Ï•(x):\n",
        "\n",
        "ðœ™\n",
        ":\n",
        "ð‘‹\n",
        "â†’\n",
        "ð»\n",
        "Ï•:Xâ†’H\n",
        "\n",
        "where\n",
        "ð»\n",
        "H is a higher-dimensional space.\n",
        "\n",
        "But instead of computing\n",
        "ðœ™\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "Ï•(x) directly, SVM only needs dot products between pairs of points:\n",
        "\n",
        "ð¾\n",
        "(\n",
        "ð‘¥\n",
        "ð‘–\n",
        ",\n",
        "ð‘¥\n",
        "ð‘—\n",
        ")=ðœ™\n",
        "(\n",
        "ð‘¥\n",
        "ð‘–\n",
        ")\n",
        "â‹…\n",
        "ðœ™\n",
        "(\n",
        "ð‘¥\n",
        "ð‘—\n",
        ")\n",
        "K(xi,xj)=Ï•(xi)â‹…Ï•(xj)\n",
        "\n",
        "Here,\n",
        "ð¾\n",
        "(\n",
        "ð‘¥\n",
        "ð‘–\n",
        ",\n",
        "ð‘¥\n",
        "ð‘—\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\tâ€‹\n",
        ",x\n",
        "j)\n",
        " is the Kernel Function â€” it computes the dot product in the high-dimensional space without explicitly computing ðœ™(ð‘¥)Ï•(x).\n",
        "\n",
        "Thatâ€™s the Kernel Trick."
      ],
      "metadata": {
        "id": "s-qHqfSv1631"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "YwK1KKA-3da-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer"
      ],
      "metadata": {
        "id": "WaoBvBEO3gRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(\" SVM Classifier Comparison on Wine Dataset\")\n",
        "print(\"--------------------------------------------\")\n",
        "print(f\"Linear Kernel Accuracy: {acc_linear:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy:    {acc_rbf:.4f}\")\n",
        "\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"\\n Linear kernel performed better.\")\n",
        "elif acc_linear < acc_rbf:\n",
        "    print(\"\\n RBF kernel performed better.\")\n",
        "else:\n",
        "    print(\"\\n  Both kernels performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSWZ8Ucy10bD",
        "outputId": "301338d8-a27b-4a6f-f324-7655d423f976"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " SVM Classifier Comparison on Wine Dataset\n",
            "--------------------------------------------\n",
            "Linear Kernel Accuracy: 0.9722\n",
            "RBF Kernel Accuracy:    1.0000\n",
            "\n",
            " RBF kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the NaÃ¯ve Bayes classifier, and why is it called \"NaÃ¯ve\"?"
      ],
      "metadata": {
        "id": "rmjYhlIZ4NYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The NaÃ¯ve Bayes classifier is a supervised machine learning algorithm based on Bayesâ€™ Theorem â€” used mainly for classification tasks, especially text classification (like spam detection, sentiment analysis, etc.).\n",
        "\n",
        "Bayesâ€™ Theorem:\n",
        "p(\n",
        "ð¶\n",
        "âˆ£\n",
        "ð‘‹\n",
        ")\n",
        "=\n",
        "ð‘ƒ\n",
        "(\n",
        "ð‘‹\n",
        "âˆ£\n",
        "ð¶\n",
        ")\n",
        "Ã—\n",
        "ð‘ƒ\n",
        "(\n",
        "ð¶\n",
        ")\n",
        "ð‘ƒ\n",
        "(\n",
        "ð‘‹\n",
        ")\n",
        "P(Câˆ£X)=\n",
        "P(X)\n",
        "P(Xâˆ£C)Ã—P(C)\n",
        "\n",
        "\tâ€‹\n",
        "P(Câˆ£X): Probability of class C given feature set X (posterior)\n",
        "\n",
        "ð‘ƒ\n",
        "(\n",
        "ð‘‹\n",
        "âˆ£\n",
        "ð¶\n",
        ")\n",
        "P(Xâˆ£C): Probability of features given class C (likelihood)\n",
        "\n",
        "ð‘ƒ\n",
        "(\n",
        "ð¶\n",
        ")\n",
        "P(C): Prior probability of class C\n",
        "\n",
        "ð‘ƒ\n",
        "(\n",
        "ð‘‹\n",
        ")\n",
        "P(X): Probability of the feature set (evidence)\n",
        "P(Câˆ£X): Probability of class C given feature set X (posterior)\n",
        "\n",
        "Calculate prior probabilities for each class (e.g., spam or not spam).\n",
        "\n",
        "Calculate likelihoods\n",
        "ð‘ƒ\n",
        "(\n",
        "ð‘‹\n",
        "âˆ£\n",
        "ð¶\n",
        ")\n",
        "P(Xâˆ£C) for each feature given the class.\n",
        "\n",
        "Use Bayesâ€™ theorem to compute posterior probabilities\n",
        "ð‘ƒ\n",
        "(\n",
        "ð¶\n",
        "âˆ£\n",
        "ð‘‹\n",
        ")\n",
        "P(Câˆ£X).\n",
        "\n",
        "Choose the class with the highest posterior probability"
      ],
      "metadata": {
        "id": "r0XRIkna4aJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian NaÃ¯ve Bayes, Multinomial NaÃ¯ve\n",
        "Bayes, and Bernoulli NaÃ¯ve Bayes\n"
      ],
      "metadata": {
        "id": "MWdJmXvo5iwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes are all variations of the Naive Bayes classification algorithm, but they differ in how they handle the type of data they are given, mainly whether features are continuous, discrete counts, or binary values:\n",
        "\n",
        "Gaussian Naive Bayes: Assumes features are continuous and distributed normally (like a bell curve). It's often used for numerical data like age or weight.\n",
        "\n",
        "Multinomial Naive Bayes: Assumes features are discrete and represented as counts. It's suitable for data where features represent the number of times something occurs within a given category, like the frequency of words in a document.\n",
        "\n",
        "Bernoulli Naive Bayes: Assumes features are binary (either present or absent). It's useful for situations where features are classified as on/off, like whether a word appears in a text."
      ],
      "metadata": {
        "id": "PxCzfvsO5mWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian NaÃ¯ve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "w_m98LvY6hoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n"
      ],
      "metadata": {
        "id": "IIIOaOmD7Bg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "gnb = GaussianNB()\n",
        "\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXymoRLc5QCb",
        "outputId": "ac55ea73-606e-4491-b5f5-a9d742cae27a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9737\n",
            "\n",
            " Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gzQQ8_z84Kbj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}